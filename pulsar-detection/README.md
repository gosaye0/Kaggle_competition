# ðŸŒŸ Pulsar Star Detection Using Scikit-learn

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A complete end-to-end machine learning project for detecting pulsar stars from radio telescope data using **Scikit-learn**. This project demonstrates professional ML engineering practices, from exploratory data analysis through model deployment.

---

## Project Overview

### The Problem
Pulsars are rapidly rotating neutron stars that emit beams of electromagnetic radiation. Radio telescopes capture millions of pulsar candidates, but only ~9% are real pulsars. Manual classification by astronomers is time-consuming and expensive.

### The Solution
Automated binary classification system that:
- Reduces false positives (saves expensive telescope observation time)
- Maintains high recall (doesn't miss real pulsars)
- Handles severe class imbalance (91:9 ratio)
- Works with 8 statistical features from radio signals

### Business Impact
**Problem Cost**: Manual verification wastes thousands of dollars in telescope time per false positive.  
**Solution Value**: Automated filtering saves 80%+ of manual verification time while maintaining 85%+ detection rate.

---

## Key Highlights

- **Comprehensive EDA**: Statistical analysis, distribution visualization, correlation analysis, outlier detection
- **Imbalance Handling**: Class weighting, SMOTE, stratified sampling
- **Use Scikit-learn**: Try different algorithms
- **Professional Evaluation**: Precision-Recall curves, ROC-AUC, confusion matrices, feature importance
- **Production-Ready Code**: Editable package structure, unit tests, configuration management
- **Deployment**: Streamlit web interface for real-time predictions

---

## ðŸ“ Project Structure

```
pulsar-detection/
â”œâ”€â”€ config.yaml                     # Hyperparameters and configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Original HTRU2.csv dataset
â”‚   â”œâ”€â”€ processed/                  # Cleaned, scaled, split data
â”‚   â””â”€â”€ external/                   # Additional metadata (optional)
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ 01_eda.ipynb                # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_train_baseline_logistic.ipynb
â”‚   â”œâ”€â”€ 04_model_comparison.ipynb
â”‚   â””â”€â”€ 05_explainability.ipynb
â”‚
â”œâ”€â”€ pulsar_detection/           # Main package (editable install)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py       # Data loading and cleaning
â”‚   â”œâ”€â”€ feature_engineering.py      # Scaling and transformations
â”‚   â”œâ”€â”€ model_training.py          # Logistic regression implementation
â”‚   â”œâ”€â”€ model_evaluation.py        # Metrics and visualizations
â”‚   â”œâ”€â”€ model_inference.py         # Prediction pipeline
â”‚   â””â”€â”€ utils.py                   # Helper functions
â”‚
â”œâ”€â”€ scripts/                        # Executable Python scripts
â”‚   â”œâ”€â”€ train.py                   # Training pipeline
â”‚   â”œâ”€â”€ predict.py                 # Inference script
â”‚   â””â”€â”€ evaluation.py              # Model evaluation
â”‚
â”œâ”€â”€ models/                         # Saved trained models
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”œâ”€â”€ test_data_preprocessing.py
â”‚   â”œâ”€â”€ test_model_training.py
â”‚   â”œâ”€â”€ test_inference.py
â”‚   â””â”€â”€ test_evaluation.py
â”‚
â”œâ”€â”€ reports/                        # Generated outputs
â”‚   â”œâ”€â”€ figures/                   # Visualizations
â”‚   â”œâ”€â”€ metrics/                   # Performance metrics
â”‚   â””â”€â”€ model_card.md             # Model documentation
â”‚
â”œâ”€â”€ app/                            # Streamlit deployment
â”‚   â”œâ”€â”€ main.py                    # Web app entry point
â”‚   â””â”€â”€ model.pkl                  # Deployed model
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ environment.yml                 # Conda environment
â”œâ”€â”€ setup.py                       # Package configuration
â”œâ”€â”€ .gitignore                     # Git ignore rules
â””â”€â”€ README.md                      # This file
```

---

## Getting Started

### Prerequisites
- Python 3.8 or higher
- pip or conda package manager
- Git (for version control)

### Installation

#### Option 1: Using pip (Recommended)

```bash
# Clone the repository
git clone https://github.com/gosaye0/kaggle_competition/pulsar-detection.git
cd pulsar-detection

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Linux/Mac:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Install package in editable mode
pip install -e .

# This installs pulsar_detection_src as importable package
```

#### Option 2: Using Conda

```bash
# Clone the repository
git clone https://github.com/gosaye0/kaggle_competition/pulsar-detection.git
cd pulsar-detection

# Create conda environment
conda env create -f environment.yml

# Activate environment
conda activate pulsar-detection

# Install package in editable mode
pip install -e .
```

### Download Dataset

1. Visit [Kaggle - HTRU2 Dataset](https://www.kaggle.com/datasets/pavanraj159/predicting-a-pulsar-star)
2. Download the dataset (requires Kaggle account)
3. Place `HTRU2.csv` in `data/raw/` folder

```bash
# Verify dataset is in place
ls data/raw/HTRU2.csv
```

---

## Usage

### 1. Exploratory Data Analysis

```bash
# Launch Jupyter Notebook
jupyter notebook

# Open notebooks/01_eda.ipynb
# Run all cells to see comprehensive data analysis
```

### 2. Training the Model

**Option A: Using Python script**
```bash
python -m scripts.train
```

**Option B: Using notebooks**
```bash
# Run notebooks/03_train_baseline_logistic.ipynb
```

### 3. Making Predictions
```bash
python -m scripts.predict --input data/raw/HTRU2.csv
```

### 4. Evaluating Model

```bash
# Generate evaluation report
python -m scripts.evaluation
```

### 5. Deploy Web App

```bash
# Navigate to app directory
cd app

# Run Streamlit app
streamlit run main.py

# App opens at http://localhost:8501
```

---

## Dataset Information

### HTRU2 Dataset
- **Source**: High Time Resolution Universe Survey
- **Samples**: 17,898 observations
- **Features**: 8 continuous features
- **Target**: Binary classification (pulsar vs non-pulsar)
- **Class Distribution**: 
  - Non-Pulsar: 16,259 (91.01%)
  - Pulsar: 1,639 (9.16%)
- **Challenge**: Severe class imbalance

### Feature Description

**Integrated Profile Features (4 features):**
1. `Mean_IP` - Mean of the integrated profile
2. `SD_IP` - Standard deviation of the integrated profile
3. `EK_IP` - Excess kurtosis of the integrated profile
4. `SK_IP` - Skewness of the integrated profile

**DM-SNR Curve Features (4 features):**  

5. `Mean_DMSNR` - Mean of the DM-SNR curve  
6. `SD_DMSNR` - Standard deviation of the DM-SNR curve  
7. `EK_DMSNR` - Excess kurtosis of the DM-SNR curve  
8. `SK_DMSNR` - Skewness of the DM-SNR curve

---

## Methodology
### 1. Exploratory Data Analysis 
- Data quality validation (no missing values, no duplicates)
- Class distribution analysis (91:9 imbalance identified)
- Feature distributions (skewness, outliers)
- Class separation analysis (Mean_IP, SD_DMSNR strongest features)
- Correlation analysis (multicollinearity in EK/SK pairs detected)
- Outlier detection (25.58% samples have outliers, kept for modeling)

### 2. Data Preprocessing (In Progress)
- Feature scaling (StandardScaler)
- Stratified train/validation/test split (60/20/20)
- Class imbalance handling (class weights)

### 3. Model Development (Upcoming)
- Dummy Classifier as a baseline model
- Try linear based models
- Upgrade to tree and forest based models

### 4. Model Evaluation (Upcoming)
- Confusion matrix
- Precision-Recall curve 
- ROC-AUC analysis
- Feature importance
- Cross-validation

### 5. Model Explainability (Upcoming)
- Coefficient interpretation
- Feature importance ranking
- Decision boundary visualization

---

## Results

*Results will be updated after model training.*

### Target Performance Metrics
- **F1-Score**: > 0.85
- **Precision**: > 0.85 (minimize false alarms - save telescope time)
- **Recall**: > 0.85 (don't miss real pulsars - maximize discoveries)
- **ROC-AUC**: > 0.95

### Why These Metrics?
- **Accuracy is misleading**: A dummy classifier achieves 91% accuracy by always predicting "non-pulsar"
- **Precision matters**: False positives waste expensive telescope observation time
- **Recall matters**: False negatives mean missing potential discoveries
- **F1-Score**: Balances precision and recall

---

## Technologies Used

| Category | Tools |
|----------|-------|
| **Language** | Python 3.10 |
| **Data Processing** | NumPy, Pandas |
| **Visualization** | Matplotlib, Seaborn |
| **Machine Learning** | Scikit-learn  |
| **Imbalance Handling** | imbalanced-learn |
| **Notebooks** | Jupyter |
| **Testing** | Pytest |
| **Code Quality** | Black, Flake8 |
| **Deployment** | Streamlit |
| **Version Control** | Git, GitHub |

---

## Key Learnings

### Technical Skills Demonstrated
1. **End-to-End ML Pipeline**: From raw data to deployed model
2. **Algorithm Implementation**: Implement scikit-learn algorithms
3. **Imbalanced Data**: Class weighting, SMOTE, metric selection
4. **Feature Engineering**: Scaling, correlation analysis, selection
5. **Professional Code**: Package structure, testing, documentation
6. **Model Evaluation**: Comprehensive metrics for imbalanced classification

### Domain Knowledge
- Understanding pulsar detection challenges
- Radio telescope data characteristics
- Trade-offs between precision and recall in astronomy
- Cost considerations in scientific research

---

## Future Improvements

- [ ] **Ensemble Methods**: Voting classifier, stacking
- [ ] **API Deployment**: FastAPI REST endpoint
- [ ] **Docker Containerization**: Reproducible deployment
- [ ] **CI/CD Pipeline**: Automated testing and deployment
- [ ] **Monitoring**: Model performance tracking in production

---

## Running Tests

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_model_training.py

# Run with coverage report
pytest --cov=pulsar_detection_src tests/

# Run with verbose output
pytest -v tests/
```

---

## License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## Acknowledgments

- **Dataset**: R. J. Lyon et al. (2016) - HTRU2 Dataset
- **HTRU Survey**: High Time Resolution Universe Survey Team
- **Kaggle**: For hosting the dataset
- **Inspiration**: Real-world astronomical classification challenges

---

## Author

**Gosaye Emshaw**

- LinkedIn: [LinkedIn Profile](https://www.linkedin.com/in/gosaye)
- GitHub: [GitHub Profile](https://github.com/gosaye0)
- Email: gosaye.work@outlook.com
- Portfolio: [Gosaye](https://github.com/gosaye0)

---

## Contact

For questions, feedback, or collaboration opportunities:
- Open an issue on GitHub
- Email: gosaye.work@outlook.com
- LinkedIn: [Gosaye](https://www.linkedin.com/in/gosaye)

---

## Show Your Support

If you find this project helpful or interesting:
- Star this repository
- Fork it for your own experiments
- Share it with others
- Report issues or suggest improvements

---

## Project Status

- [x] Project setup and structure
- [x] Exploratory Data Analysis (EDA)
- [ ] Data preprocessing pipeline
- [ ] Model training and evaluation
- [ ] Model explainability analysis
- [ ] Streamlit deployment
- [ ] Unit tests
- [ ] Documentation completion

**Last Updated**: 10/7/2025  
**Current Phase**: Data Preprocessing  
**Overall Progress**: ~20% Complete

---

**Built for Machine Learning and Data Science community**